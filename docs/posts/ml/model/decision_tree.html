<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Decision Trees Explained with Just 10 Rows</title>

    <!-- Google tag (gtag.js) -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-HCYYKZC4C0"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-HCYYKZC4C0");
    </script>

    <style>
      body {
        font-family: sans-serif;
        background: #e9ecf5;
        margin: 0;
        padding: 16px;
      }
      .tile {
        background: #ffffff;
        padding: 20px;
        max-width: 1100px;
        margin: 15px auto;
      }

      h1,
      h2,
      h3,
      h4 {
        font-family:
          system-ui,
          -apple-system,
          BlinkMacSystemFont,
          "Segoe UI",
          sans-serif;
        font-weight: 600;
        margin: 0;
      }

      h1 {
        font-size: 2.2rem;
        margin-top: 0;
        margin-bottom: 1rem;
        border-left: 8px solid #1a4d8f;
        padding-left: 12px;
        color: #102c52;
      }

      h2 {
        font-size: 1.7rem;
        margin-top: 2rem;
        margin-bottom: 0.7rem;
        border-left: 6px solid #2d9c5a;
        padding-left: 10px;
        color: #1e6b3a;
      }

      h3 {
        font-size: 1.35rem;
        margin-top: 1.6rem;
        margin-bottom: 0.5rem;
        border-left: 4px solid #c97a19;
        padding-left: 8px;
        color: #8a520f;
      }

      h4 {
        font-size: 1.15rem;
        margin-top: 1.2rem;
        margin-bottom: 0.4rem;
        border-left: 3px solid #b72f38;
        padding-left: 6px;
        color: #7a1f24;
      }

      p {
        color: #3f4a5f;
      }
      pre {
        background: #f3f4fa;
        padding: 10px;
        border: 1px solid #d7d9e4;
        overflow-x: auto;
        color: #2b2b2b;
      }
      @media (max-width: 600px) {
        body {
          padding: 8px;
        }
        .tile {
          padding: 16px;
          margin: 12px auto;
        }
      }

      table {
        width: 100%;
        border-collapse: collapse;
        margin: 12px 0;
        font-size: 15px;
        background: #ffffff;
      }

      thead th {
        background: #e5e8f3;
        color: #2b3a67;
        font-weight: bold;
        padding: 10px;
        border-bottom: 2px solid #c7ccdd;
      }

      tbody td {
        padding: 8px 10px;
        border-bottom: 1px solid #d7d9e4;
        color: #3f4a5f;
      }
      table th,
      table td {
        text-align: center;
      }

      tbody tr:nth-child(even) {
        background: #f7f8fc;
      }
    </style>
  </head>
  <body>
    <div class="tile">
      <h1>Decision Trees Explained with Just 10 Rows</h1>
    </div>

    <!------------------------------------------------------------------------------------>

    <div class="tile">
      <h2>Introduction</h2>

      <p>
        In this article, we explain how a decision tree chooses split points and
        how the tree grows, based on the popular algorithm
        <b>CART (Classification and Regression Tree)</b>. We use only 10 rows of
        data to show the process in a clear and simple way.
      </p>

      <p>
        Decision trees are the basic structure behind many strong models used
        today, such as Random Forest and GBDT methods (XGBoost / LightGBM /
        CatBoost).
      </p>

      <p>
        Because of this, understanding how a decision tree works is an important
        step toward understanding more advanced models.
      </p>

      <h4>Decision Tree Overview</h4>
      <pre>

Decision Tree
   ├── Random Forest (many decision trees used in parallel)
   └── Gradient Boosting Models (trees added one by one to improve the model)
         ├── XGBoost
         ├── LightGBM
         └── CatBoost

</pre
      >
    </div>

    <!------------------------------------------------------------------------------------>

    <div class="tile">
      <h2>What Is a Decision Tree?</h2>

      <p>
        A decision tree is a model that asks a series of “yes / no” questions to
        the data. Based on each answer, the data splits into different branches,
        and the tree grows from the top to the bottom.
      </p>

      <p>
        At each step, the model chooses one question (a split) from many
        possible questions. It picks the question that separates the data in the
        best way. By repeating this process, similar data points end up on the
        same branch, and different data points naturally move to different
        branches.
      </p>

      <p>
        When you reach the end of a branch — the leaf — the model gives a
        prediction based on that leaf.
      </p>

      <pre>

root node
└── split age ≤ 10 ?
    ├── internal node
    │   └── split favorite_color = "blue" ?
    │       ├── internal node
    │       │   └── split has_pet = yes ?
    │       │       ├── leaf → "Outdoor play"
    │       │       └── leaf → "Arts & crafts"
    │       └── leaf → "Puzzle games"
    └── internal node
        └── split height ≥ 160 ?
            ├── leaf → "Sports activities"
            └── leaf → "Board games"


</pre
      >
    </div>

    <div class="tile">
      <h2>Dataset Used in This Example</h2>

      <p>
        In this article, we explain decision trees using only the 10 data points
        shown below. As an example, we build a simple decision tree model that
        predicts whether a person has financial risk or not.
      </p>

      <h3>Data</h3>

      <table>
        <thead>
          <tr>
            <th>age</th>
            <th>income</th>
            <th>occupation_code</th>
            <th>default_flag ( class )</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>19</td>
            <td>5850</td>
            <td>2</td>
            <td>1</td>
          </tr>
          <tr>
            <td>22</td>
            <td>12000</td>
            <td>1</td>
            <td>1</td>
          </tr>
          <tr>
            <td>27</td>
            <td>16900</td>
            <td>1</td>
            <td>1</td>
          </tr>
          <tr>
            <td>27</td>
            <td>55250</td>
            <td>3</td>
            <td>0</td>
          </tr>
          <tr>
            <td>30</td>
            <td>42000</td>
            <td>2</td>
            <td>0</td>
          </tr>
          <tr>
            <td>35</td>
            <td>32500</td>
            <td>4</td>
            <td>0</td>
          </tr>
          <tr>
            <td>41</td>
            <td>18000</td>
            <td>2</td>
            <td>1</td>
          </tr>
          <tr>
            <td>45</td>
            <td>0</td>
            <td>3</td>
            <td>0</td>
          </tr>
          <tr>
            <td>52</td>
            <td>39000</td>
            <td>1</td>
            <td>0</td>
          </tr>
          <tr>
            <td>58</td>
            <td>61000</td>
            <td>4</td>
            <td>0</td>
          </tr>
        </tbody>
      </table>

      <h4>occupation_code</h4>

      <pre>
1  Student
2  Office worker
3  Self-employed
4  Unemployed
</pre
      >
    </div>

    <div class="tile">
      <h1>Table of Contents</h1>

      <h3>Decision Tree Process</h3>
      <ul>
        <li>Step 1: Listing All Possible Split Conditions</li>
        <li>Step 2: Calculating the Weighted Gini Impurity</li>
        <li>Step 3: Choosing the Split Question</li>
        <li>Step 4: Growing the Tree Further (Part 1)</li>
        <li>Step 5: Growing the Tree Further (Part 2)</li>
        <li>Step 6: Final Model</li>
      </ul>

      <h3>Applications of Decision Trees</h3>
      <ul>
        <li>Gain</li>
        <li>Output Patterns</li>
        <li>Pruning</li>
        <li>Common Questions</li>
      </ul>
    </div>

    <!------------------------------------------------------------------------------------>

    <div class="tile">
      <h1>Decision Tree Process</h1>
    </div>

    <div class="tile">
      <h2>Step 1: Listing All Possible Split Conditions</h2>

      <p>
        A decision tree is very simple.<br />
        From many possible <code>if</code> conditions, it just chooses
        <b>the condition that splits the data the best</b>, one step at a time.
      </p>

      <p>
        In Step 1, we list all possible <code>if</code> conditions.<br />
        This is the process of listing every split pattern the tree can try.
      </p>

      <h3>How to List All Split Patterns</h3>

      <p>
        In a decision tree, the way we create split candidates is different for
        numerical features and categorical features.
      </p>

      <h4>1. Numerical Features (age, income)</h4>

      <p>
        For numerical features, we consider splits only at the
        <b>midpoints between neighboring values</b>.<br />
        No matter where you place a boundary between two values, the data on
        each side will be the same, so using the midpoint is enough.
      </p>

      <p>Example: If we sort the age values in ascending order, we get:</p>

      <pre>
19, 22, 27, 27, 30, …
</pre
      >

      <p>
        If we want to split between 19 and 22, we use the following midpoint:
      </p>

      <pre>
midpoint = (19 + 22) / 2 = 20.5
</pre
      >

      <p>
        CART then uses this value as a threshold and creates the following split
        candidate:
      </p>

      <pre>
if age ≤ 20.50:
</pre
      >

      <h4>2. Categorical Feature (occupation)</h4>

      <p>
        For categorical features, we cannot define midpoints.<br />
        So in CART, we create splits by forming
        <b>subsets of categories for the Left and Right groups</b>.
      </p>

      <p>In our example, the feature “occupation” has four categories:</p>

      <pre>
{Student, Office worker, Self-employed, Unemployed}
</pre
      >

      <p>
        All subsets of these four categories give
        <code>2⁴ = 16</code> combinations.<br />
        But we remove the two cases where one side is empty, and we also treat
        Left/Right swaps as the same condition.<br />
        As a result, we only need to list <b>7 effective patterns</b>.
      </p>

      <h3>List of Split Patterns (25 Patterns)</h3>

      <pre>
if age <= 24.50:
if age <= 27.00:
if age <= 28.50:
if age <= 32.50:
if age <= 38.00:
if age <= 43.00:
if age <= 48.50:
if age <= 55.00:
if income <= 2925.00:
if income <= 8925.00:
if income <= 14450.00:
if income <= 17450.00:
if income <= 25250.00:
if income <= 35750.00:
if income <= 40500.00:
if income <= 48625.00:
if income <= 58125.00:
Left = {Student},          Right = {Office worker, Self-employed, Unemployed}
Left = {Office worker},    Right = {Student, Self-employed, Unemployed}
Left = {Self-employed},    Right = {Student, Office worker, Unemployed}
Left = {Unemployed},       Right = {Student, Office worker, Self-employed}
Left = {Student, Office worker},          Right = {Self-employed, Unemployed}
Left = {Student, Self-employed},          Right = {Office worker, Unemployed}
Left = {Student, Unemployed},             Right = {Office worker, Self-employed}
</pre
      >

      <p>
        Now we have all 25 split patterns that can be used in this example.<br />
        In the next step, we will evaluate these patterns and find which one is
        the best first split for the decision tree.
      </p>
    </div>

    <!------------------------------------------------------------------------------------>

    <div class="tile">
      <h2>Step 2: Calculating the Weighted Gini Impurity</h2>

      <p>
        To find which split condition is the best, we use the weighted Gini
        impurity.<br />
        To compute this value, we first need to calculate the Gini impurity
        itself.<br />
        For each split candidate, we compute Gini-left and Gini-right.
      </p>

      <h3>What Is Gini?</h3>

      <p>
        <b>Gini impurity</b> shows how mixed the classes (such as 0/1) are
        inside a group.<br />
        The more mixed the group is, the higher the value becomes.<br />
        The more the group is filled with only one class, the lower the value
        becomes.
      </p>

      <ul>
        <li><b>All samples are the same class → Gini = 0 (cleanest)</b></li>
        <li><b>Half 0 and half 1 → Gini = 0.5 (most mixed)</b></li>
      </ul>

      <p>
        In short, Gini tells us <b>how pure</b> a group is.<br />
        In a decision tree, a split is considered better if it creates groups
        that are more pure.<br /><br />
        NOTE: Gini = 0.5 is the most mixed case for binary classification.<br />
        In binary tasks, the Gini value always stays between 0 and 0.5.
      </p>

      <h3>What Do We Want to Achieve with Gini?</h3>

      <p>
        The goal of a decision tree is to split the data into left and right
        groups and create
        <b>the cleanest groups possible, with little class mixing</b>.
      </p>

      <p>So for each split candidate, we check three things:</p>

      <ul>
        <li>How mixed the left group is (Gini-left)</li>
        <li>How mixed the right group is (Gini-right)</li>
        <li>We take a weighted average based on the size of each group</li>
      </ul>

      <p>
        This weighted average is the
        <b>weighted Gini</b>.<br />
        It lets us compare splits fairly by considering the size of each side.
      </p>

      <p>
        <b
          >Split with the smallest weighted Gini = the split that separates the
          classes the best</b
        ><br />
        The decision tree chooses this split.
      </p>

      <h3>Calculating Gini (Gini-Left and Gini-Right)</h3>

      <p>
        When we split the data using a question, it is divided into
        <b>Left</b> (samples that meet the condition) and <b>Right</b> (samples
        that do not).<br />
        In a decision tree, we evaluate the purity of these two nodes — how well
        the classes are separated — by calculating
        <b>Gini<sub>left</sub></b> and <b>Gini<sub>right</sub></b> as follows:
      </p>

      <p>
        Gini<sub>left</sub> = 1 − p<sub>left</sub><sup>2</sup> − (1 −
        p<sub>left</sub>)<sup>2</sup><br />
        Gini<sub>right</sub> = 1 − p<sub>right</sub><sup>2</sup> − (1 −
        p<sub>right</sub>)<sup>2</sup>
      </p>

      <p>
        Here, p<sub>left</sub> and p<sub>right</sub> are the
        <b>proportion of class 1</b> in the Left node and Right node,
        respectively.
      </p>

      <p>In this example, we define:</p>

      <pre>
class 1 = default occurred
class 0 = no default
</pre
      >

      <p>
        For each group made by a question, we check what fraction of the samples
        are default (class 1) and what fraction are non-default (class 0).
      </p>

      <p>
        We have 25 questions in total.<br />
        For each of them, we calculate 25 values of Gini-left and 25 values of
        Gini-right.
      </p>

      <p>
        Next, let’s look at Gini-left and Gini-right in detail for the question
        <code>if income &lt;= 25250:</code>.
      </p>

      <h4>Gini-Left</h4>

      <pre>
In the group where income is 25,250 or less, there are 5 samples:
class = 1 → 4 samples
class = 0 → 1 sample

The Gini-left value for this group is 0.32.
</pre
      >

      <p>
        Gini<sub>left</sub> = 1 − (4/5)<sup>2</sup> − (1/5)<sup>2</sup> = 0.32
      </p>

      <table>
        <thead>
          <tr>
            <th>age</th>
            <th>income</th>
            <th>occupation_code</th>
            <th>default_flag</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>19</td>
            <td>5850</td>
            <td>2</td>
            <td>1</td>
          </tr>
          <tr>
            <td>22</td>
            <td>12000</td>
            <td>1</td>
            <td>1</td>
          </tr>
          <tr>
            <td>27</td>
            <td>16900</td>
            <td>1</td>
            <td>1</td>
          </tr>
          <tr>
            <td>41</td>
            <td>18000</td>
            <td>2</td>
            <td>1</td>
          </tr>
          <tr>
            <td>45</td>
            <td>0</td>
            <td>3</td>
            <td>0</td>
          </tr>
        </tbody>
      </table>

      <h4>Gini-Right</h4>

      <pre>
In the group where income is greater than 25,250, there are 5 samples:
class = 0 → 5 samples
class = 1 → 0 samples

The Gini-right value for this group is 0.
This means the right group is completely pure.
</pre
      >

      <p>
        Gini<sub>right</sub> = 1 − (0/5)<sup>2</sup> − (5/5)<sup>2</sup> = 0
      </p>

      <table>
        <thead>
          <tr>
            <th>age</th>
            <th>income</th>
            <th>occupation_code</th>
            <th>default_flag (class)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>27</td>
            <td>55250</td>
            <td>3</td>
            <td>0</td>
          </tr>
          <tr>
            <td>30</td>
            <td>42000</td>
            <td>2</td>
            <td>0</td>
          </tr>
          <tr>
            <td>35</td>
            <td>32500</td>
            <td>4</td>
            <td>0</td>
          </tr>
          <tr>
            <td>52</td>
            <td>39000</td>
            <td>1</td>
            <td>0</td>
          </tr>
          <tr>
            <td>58</td>
            <td>61000</td>
            <td>4</td>
            <td>0</td>
          </tr>
        </tbody>
      </table>

      <h3>Calculating the Weighted Gini Impurity</h3>
      <p>
        Finally, we use Gini-left and Gini-right to compute the weighted Gini
        impurity.
      </p>

      <p>
        w = (N<sub>left</sub> / N) × Gini<sub>left</sub> + (N<sub>right</sub> /
        N) × Gini<sub>right</sub>
      </p>

      <pre>
N = number of samples before the split
N<sub>left</sub> = number of samples in the Left group
N<sub>right</sub> = number of samples in the Right group
</pre
      >

      <pre>
weighted Gini impurity = (5/10)*0.32 + (5/10)*0 = 0.16
</pre
      >

      <p>
        For the split <code>if income &lt;= 25250:</code>,<br />
        the weighted Gini impurity is 0.16.
      </p>

      <p>
        When we calculate the weighted Gini impurity for all other splits, we
        get the following results.
      </p>

      <p>In total, we compute 25 weighted Gini impurity values.</p>

      <pre>
if age <= 20.50:  # gL=0.0000, gR=0.4444, wG=0.4000
if age <= 24.50:  # gL=0.0000, gR=0.3750, wG=0.3000
if age <= 27.00:  # gL=0.3750, gR=0.2778, wG=0.3167
if age <= 28.50:  # gL=0.3750, gR=0.2778, wG=0.3167
if age <= 32.50:  # gL=0.4800, gR=0.3200, wG=0.4000
if age <= 38.00:  # gL=0.5000, gR=0.3750, wG=0.4500
if age <= 43.00:  # gL=0.4898, gR=0.0000, wG=0.3429
if age <= 48.50:  # gL=0.5000, gR=0.0000, wG=0.4000
if age <= 55.00:  # gL=0.4938, gR=0.0000, wG=0.4444
if income <= 2925.00:  # gL=0.0000, gR=0.4938, wG=0.4444
if income <= 8925.00:  # gL=0.5000, gR=0.4688, wG=0.4750
if income <= 14450.00:  # gL=0.4444, gR=0.4082, wG=0.4190
if income <= 17450.00:  # gL=0.3750, gR=0.2778, wG=0.3167
if income <= 25250.00:  # gL=0.3200, gR=0.0000, wG=0.1600
if income <= 35750.00:  # gL=0.4444, gR=0.0000, wG=0.2667
if income <= 40500.00:  # gL=0.4898, gR=0.0000, wG=0.3429
if income <= 48625.00:  # gL=0.5000, gR=0.0000, wG=0.4000
if income <= 58125.00:  # gL=0.4938, gR=0.0000, wG=0.4444
Left = ['Office worker'], Right = ['Self-employed', 'Student', 'Unemployed']  # gL=0.4444, gR=0.4082, wG=0.4190
Left = ['Self-employed'], Right = ['Office worker', 'Student', 'Unemployed']  # gL=0.0000, gR=0.5000, wG=0.4000
Left = ['Student'], Right = ['Office worker', 'Self-employed', 'Unemployed']  # gL=0.4444, gR=0.4082, wG=0.4190
Left = ['Unemployed'], Right = ['Office worker', 'Self-employed', 'Student']  # gL=0.0000, gR=0.5000, wG=0.4000
Left = ['Office worker', 'Self-employed'], Right = ['Student', 'Unemployed']  # gL=0.4800, gR=0.4800, wG=0.4800
Left = ['Office worker', 'Student'], Right = ['Self-employed', 'Unemployed']  # gL=0.4444, gR=0.0000, wG=0.2667
Left = ['Office worker', 'Unemployed'], Right = ['Self-employed', 'Student']  # gL=0.4800, gR=0.4800, wG=0.4800
</pre
      >

      <details>
        <summary>OPEN / CLOSE</summary>

        <pre>
import itertools

# ====== Data ======
data = [
    {"age": 19, "income": 5850, "occ": "Office worker", "y": 1},
    {"age": 22, "income": 12000, "occ": "Student", "y": 1},
    {"age": 27, "income": 16900, "occ": "Student", "y": 1},
    {"age": 27, "income": 55250, "occ": "Self-employed", "y": 0},
    {"age": 30, "income": 42000, "occ": "Office worker", "y": 0},
    {"age": 35, "income": 32500, "occ": "Unemployed", "y": 0},
    {"age": 41, "income": 18000, "occ": "Office worker", "y": 1},
    {"age": 45, "income": 0, "occ": "Self-employed", "y": 0},
    {"age": 52, "income": 39000, "occ": "Student", "y": 0},
    {"age": 58, "income": 61000, "occ": "Unemployed", "y": 0},
]

# continuous features
num_features = ["age", "income"]
# categorical feature (one feature only here)
cat_feature = "occ"


# =========================
# Gini
# =========================
def gini(subset):
    if len(subset) == 0:
        return 0
    p1 = sum(r["y"] for r in subset) / len(subset)
    return 1 - p1 * p1 - (1 - p1) * (1 - p1)


# =========================
# numerical split midpoints
# =========================
def midpoints(values):
    values = sorted(values)
    mids = []
    for a, b in zip(values, values[1:]):
        mids.append((a + b) / 2)
    return mids


# =========================
# categorical splits
# =========================
occ_values = sorted(list({row[cat_feature] for row in data}))


def cart_category_splits(categories):
    k = len(categories)
    valid_left_sets = []
    seen = set()

    for r in range(1, k):  # subset sizes 1..3
        for comb in itertools.combinations(categories, r):

            left = set(comb)
            right = set(categories) - left
            if len(left) == 0 or len(right) == 0:
                continue

            left_key = tuple(sorted(left))
            right_key = tuple(sorted(right))

            # avoid symmetric duplicates
            if right_key in seen:
                continue

            valid_left_sets.append(left_key)
            seen.add(left_key)

    return valid_left_sets


cat_splits = cart_category_splits(occ_values)


# =========================
# compute weighted Gini impurity
# =========================
N = len(data)

# ===== numerical splits =====
for feat in num_features:
    mids = midpoints([row[feat] for row in data])
    for m in mids:
        left = [r for r in data if r[feat] <= m]
        right = [r for r in data if r[feat] > m]

        gL = gini(left)
        gR = gini(right)
        w = (len(left) / N) * gL + (len(right) / N) * gR

        print(f"if {feat} <= {m:.2f}:  # gL={gL:.4f}, gR={gR:.4f}, wG={w:.4f}")

# ===== categorical splits =====
for left_tuple in cat_splits:
    left_set = set(left_tuple)
    right_set = set(occ_values) - left_set

    left = [r for r in data if r[cat_feature] in left_set]
    right = [r for r in data if r[cat_feature] in right_set]

    gL = gini(left)
    gR = gini(right)
    w = (len(left) / N) * gL + (len(right) / N) * gR

    print(
        f"Left = {sorted(left_set)}, Right = {sorted(right_set)}"
        f"  # gL={gL:.4f}, gR={gR:.4f}, wG={w:.4f}"
    )
</pre
        >
      </details>
    </div>

    <!------------------------------------------------------------------------------------>

    <div class="tile">
      <h2>Step 3: Choosing the Split Question</h2>

      <p>
        The question with the smallest weighted Gini impurity is chosen as the
        first split.<br />
        A smaller weighted Gini impurity means the split separates the classes
        (here, whether a person defaults or not) more effectively.
      </p>

      <p>
        In this example, the following split has the smallest weighted Gini
        impurity:
      </p>

      <pre>
if income <= 25250.00:  # gL=0.3200, gR=0.0000, wG=0.1600
</pre
      >

      <p>The decision tree selects this question:</p>

      <pre>
income ≤ 25250 ?
├── Yes (low)
└── No (high)
</pre
      >

      <p>This split divides the data into the following two groups.</p>

      <pre>

Low
| age | income | occupation_code | default_flag |
| --- | ------ | --------------- | ------------ |
| 19  | 5,850  | 2               | 1            |
| 22  | 12,000 | 1               | 1            |
| 27  | 16,900 | 1               | 1            |
| 41  | 18,000 | 2               | 1            |
| 45  | 0      | 3               | 0            |

High
| age | income | occupation_code | default_flag |
| --- | ------ | --------------- | ------------ |
| 27  | 55,250 | 3               | 0            |
| 30  | 42,000 | 2               | 0            |
| 35  | 32,500 | 4               | 0            |
| 52  | 39,000 | 1               | 0            |
| 58  | 61,000 | 4               | 0            |

</pre
      >
    </div>
    <!------------------------------------------------------------------------------------>

    <div class="tile">
      <h2>Step 4: Growing the Tree Further (Part 1)</h2>

      <p>
        In Step 3, we selected the first split. Now, we choose the next question
        for the Yes (low) group.
      </p>

      <p>
        The procedure is exactly the same as before:
        <br />
        [Step 1: List all possible split questions]
        <br />
        [Step 2: Calculate the weighted Gini impurity]
        <br />
        [Step 3: Choose the best question]
      </p>

      <p>
        When we compute the weighted Gini impurity for the Yes (low) group, we
        get the following results.
      </p>

      <pre>
if age <= 20.50:  # gL=0.0000, gR=0.3750, wG=0.3000
if age <= 24.50:  # gL=0.0000, gR=0.4444, wG=0.2667
if age <= 34.00:  # gL=0.0000, gR=0.5000, wG=0.2000
if age <= 43.00:  # gL=0.0000, gR=0.0000, wG=0.0000
if income <= 2925.00:  # gL=0.0000, gR=0.0000, wG=0.0000
if income <= 8925.00:  # gL=0.5000, gR=0.0000, wG=0.2000
if income <= 14450.00:  # gL=0.4444, gR=0.0000, wG=0.2667
if income <= 17450.00:  # gL=0.3750, gR=0.0000, wG=0.3000
Left = ['Office worker'], Right = ['Self-employed', 'Student']  # gL=0.0000, gR=0.4444, wG=0.2667
Left = ['Self-employed'], Right = ['Office worker', 'Student']  # gL=0.0000, gR=0.0000, wG=0.0000
Left = ['Student'], Right = ['Office worker', 'Self-employed']  # gL=0.0000, gR=0.4444, wG=0.2667
</pre
      >

      <p>
        In this case, the following question is selected, because it has a
        weighted Gini of 0.0000.
      </p>

      <pre>
if age <= 43.00:  # gL=0.0000, gR=0.0000, wG=0.0000
if income <= 2925.00:  # gL=0.0000, gR=0.0000, wG=0.0000
Left = ['Self-employed'], Right = ['Office worker', 'Student']  # gL=0.0000, gR=0.0000, wG=0.0000
</pre
      >

      <p>
        When multiple questions have a weighted Gini of 0.0000, we cannot decide
        which one is better based on the data alone.<br />
        Any of them will separate the classes perfectly.
      </p>

      <p>Here, we simply choose the first age-related question.</p>

      <p>With this choice, the tree grows as follows.</p>

      <pre>
income ≤ 25250 ?
├── Yes
│   └── age ≤ 43 ?
│       ├── Yes → predict 1
│       └── No  → predict 0
└── No
    └── This part will be created in Step 5
</pre
      >
    </div>

    <!------------------------------------------------------------------------------------>

    <div class="tile">
      <h2>Step 5: Growing the Tree Further (Part 2)</h2>

      <p>
        Next, we choose the best question for the No (high) group.<br />
        The procedure is exactly the same as in Step 4.<br />
        When we calculate the weighted Gini impurity for the No (high) group, we
        get the following results.
      </p>

      <pre>
if age <= 32.50:  # gL=0.0000, gR=0.0000, wG=0.0000
if age <= 43.50:  # gL=0.0000, gR=0.0000, wG=0.0000
if age <= 55.00:  # gL=0.0000, gR=0.0000, wG=0.0000
if income <= 35750.00:  # gL=0.0000, gR=0.0000, wG=0.0000
if income <= 40500.00:  # gL=0.0000, gR=0.0000, wG=0.0000
if income <= 48625.00:  # gL=0.0000, gR=0.0000, wG=0.0000
if income <= 58125.00:  # gL=0.0000, gR=0.0000, wG=0.0000

Left = ['Office worker'], Right = ['Self-employed', 'Student', 'Unemployed']  # gL=0.0000, gR=0.0000, wG=0.0000
Left = ['Self-employed'], Right = ['Office worker', 'Student', 'Unemployed']  # gL=0.0000, gR=0.0000, wG=0.0000
Left = ['Student'], Right = ['Office worker', 'Self-employed', 'Unemployed']  # gL=0.0000, gR=0.0000, wG=0.0000
Left = ['Unemployed'], Right = ['Office worker', 'Self-employed', 'Student']  # gL=0.0000, gR=0.0000, wG=0.0000
Left = ['Office worker', 'Self-employed'], Right = ['Student', 'Unemployed']  # gL=0.0000, gR=0.0000, wG=0.0000
Left = ['Office worker', 'Student'], Right = ['Self-employed', 'Unemployed']  # gL=0.0000, gR=0.0000, wG=0.0000
Left = ['Office worker', 'Unemployed'], Right = ['Self-employed', 'Student']  # gL=0.0000, gR=0.0000, wG=0.0000
</pre
      >

      <p>
        All weighted Gini impurity values were 0.0000.<br />
        This means that everyone who does not meet the condition
        <code>income ≤ 25250</code> has no default in this dataset.<br />
        In other words, the model does not need to make any further decisions,
        so the tree stops growing at this point.
      </p>
    </div>

    <!------------------------------------------------------------------------------------>

    <div class="tile">
      <h2>Step 6: Final Model</h2>

      <pre>
income ≤ 25250 ?
├── Yes
│   └── age ≤ 43 ?
│       ├── Yes → predict 1
│       └── No  → predict 0
└── No
    └── predict 0
        </pre
      >
    </div>

    <!------------------------------------------------------------------------------------>

    <div class="tile">
      <h1>Applications of Decision Trees</h1>
    </div>

    <!------------------------------------------------------------------------------------>
    <div class="tile">
      <h2>Gain</h2>

      <p>
        In a decision tree, <b>Gain</b> is a value that shows “how much the
        impurity (Gini) decreases when the data is split using a certain
        feature.”
      </p>

      <p>
        As we have seen, Gain is used to decide which split condition to choose.
        In CART classification trees, the Gini impurity is commonly used, but
        other measures such as entropy also exist.
      </p>

      <p>
        Different tree-based models use different impurity measures, but they
        all share the same idea: <br />&nbsp;&nbsp;&nbsp;&nbsp;they calculate
        the <b>Gain</b> (the improvement from a split) and select the split with
        the highest Gain.
      </p>

      <p>A larger Gain means the split is considered “better.”</p>

      <p>
        Other tree-based models may not use Gini, but the process of calculating
        Gain and choosing the best split is the same. The only difference is
        <b>what type of improvement they measure</b>. In GBDT models, for
        example, the Gain is the amount of decrease in the loss.
      </p>

      <table border="1" cellpadding="6">
        <tr>
          <th>Model</th>
          <th>Gain</th>
        </tr>

        <tr>
          <td>Decision Tree (CART)</td>
          <td>Impurity reduction (Gini decrease)</td>
        </tr>

        <tr>
          <td>Random Forest (CART)</td>
          <td>Impurity reduction (Gini decrease)</td>
        </tr>

        <tr>
          <td>LightGBM</td>
          <td>
            Loss reduction (based on gradients and second-order derivatives)
          </td>
        </tr>

        <tr>
          <td>XGBoost</td>
          <td>
            Loss reduction (based on gradients and second-order derivatives)
          </td>
        </tr>

        <tr>
          <td>CatBoost</td>
          <td>Loss reduction (gradient-based, considers target encoding)</td>
        </tr>
      </table>
    </div>
    <!------------------------------------------------------------------------------------>

    <div class="tile">
      <h2>Output Patterns (CART)</h2>

      <p>
        In this example, we looked at a binary classification task (financial
        risk: 2 classes).<br />
        In practice, decision trees can be used for several types of tasks.<br />
        The basic idea is almost the same for all tasks. The only differences
        are the final output format and how impurity is calculated.
      </p>

      <table>
        <thead>
          <tr>
            <th>Task</th>
            <th>Description</th>
            <th>Final Output</th>
            <th>How Impurity Is Calculated</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><b>Regression</b></td>
            <td>Predicts a continuous value</td>
            <td>Mean value in the leaf</td>
            <td>Measures prediction error (uses mean squared error)</td>
          </tr>

          <tr>
            <td><b>Binary Classification</b></td>
            <td>Predicts 0 or 1</td>
            <td>Majority class in the leaf</td>
            <td>Weighted Gini impurity</td>
          </tr>

          <tr>
            <td><b>Multi-class Classification</b></td>
            <td>Three or more classes</td>
            <td>Majority class in the leaf</td>
            <td>Weighted Gini impurity</td>
          </tr>

          <tr>
            <td><b>Probability Estimation</b></td>
            <td>Predicts the probability of each class</td>
            <td>Relative class frequency in the leaf</td>
            <td>Weighted Gini impurity</td>
          </tr>
        </tbody>
      </table>
    </div>

    <div class="tile">
      <h2>Pruning</h2>

      <p>
        <b>Pruning</b> is a way to prevent a decision tree from overfitting. It
        controls how the tree grows and “cuts off” branches that are not needed.
      </p>

      <p>There are two types of pruning:</p>

      <ul>
        <li>
          <b>Pre-pruning</b>: Stop the split while the tree is still growing.
        </li>
        <li>
          <b>Post-pruning</b>: Grow the tree fully first, then remove branches
          that are not useful.
        </li>
      </ul>

      <h3>Pre-Pruning</h3>

      <p>
        <b>Pre-pruning</b> stops a split while the tree is still growing, by
        deciding “we should not split here.”
      </p>

      <p>It prevents further growth using rules (thresholds) such as:</p>

      <ul>
        <li>the Gain is too small,</li>
        <li>the number of samples is too low,</li>
        <li>the tree is becoming too deep, etc.</li>
      </ul>

      <p>
        This helps stop the tree from becoming unnecessarily deep and keeps the
        computation light. It also helps reduce overfitting to some degree.
      </p>

      <p>
        However, there is a downside: if the decision is made too early, the
        tree may stop a split that was actually important.
      </p>

      <table border="1" cellpadding="6">
        <tr>
          <thead>
            <th>Parameter Name<br />(scikit-learn)</th>
            <th>Summary</th>
            <th>Technical Explanation</th>
          </thead>
        </tr>

        <!-- max_depth -->
        <tr>
          <td>max_depth</td>
          <td>Limits the depth of the tree</td>
          <td>
            Sets the maximum depth (number of splits) the tree is allowed to
            grow.<br />
            This stops the tree early.
          </td>
        </tr>

        <!-- min_samples_split -->
        <tr>
          <td>min_samples_split</td>
          <td>Minimum samples required to split</td>
          <td>
            Before splitting a node, the number of samples inside the node must
            be<br />
            at least this value. If the number is smaller, the split will not
            happen.
          </td>
        </tr>

        <!-- min_samples_leaf -->
        <tr>
          <td>min_samples_leaf</td>
          <td>Minimum samples required in each leaf</td>
          <td>
            If a split would create a child node with fewer samples than this
            value,<br />
            that split is not allowed.
          </td>
        </tr>

        <!-- max_leaf_nodes -->
        <tr>
          <td>max_leaf_nodes</td>
          <td>Limits the number of leaf nodes</td>
          <td>Sets an upper limit on how many leaf nodes the tree can have.</td>
        </tr>

        <!-- max_features -->
        <tr>
          <td>max_features</td>
          <td>Limits the number of features used for splitting</td>
          <td>
            Restricts how many features can be considered when looking for the
            best split.<br />
            Using only a subset of features can improve generalization.
          </td>
        </tr>

        <!-- min_impurity_decrease -->
        <tr>
          <td>min_impurity_decrease</td>
          <td>Split only if impurity decreases by a certain amount</td>
          <td>
            A split is performed only if the decrease in impurity (the Gain) is
            greater than or equal to this value.
          </td>
        </tr>
      </table>

      <h3>Post-Pruning</h3>

      <pre>
Post-pruning grows the tree fully (or at least deeply) first,
and then removes unnecessary branches afterward.
Because the tree is already complete, we can carefully check
which branches are truly unnecessary. This is the biggest advantage.

It is considered the most accurate and theoretically strong method,
but it requires more computation.
</pre
      >

      <p>
        As a decision tree keeps splitting, it becomes more complex and may fit
        the training data too closely — this is overfitting.<br />
        To prevent this, CART applies a “penalty” when the tree becomes too
        large and removes branches that do not give enough benefit. This process
        is called <b>pruning</b>.
      </p>

      <p>After growing the tree large enough, CART evaluates:</p>

      <ul>
        <li>Error (how well the tree fits the training data)</li>
        <li>
          Penalty for tree size (a penalty based on how many branches it has)
        </li>
      </ul>

      <p>
        It then combines these values and cuts branches that do not give enough
        improvement compared to the penalty. This makes the tree simpler and
        better at generalization.
      </p>

      <p>
        The method used for this is
        <b>cost-complexity pruning</b>, where the penalty grows as the tree
        becomes larger.
      </p>
    </div>

    <!------------------------------------------------------------------------------------>

    <div class="tile">
      <h2>Common Questions</h2>

      <h3>Q1: Why do we split using the midpoint?</h3>
      <p>
        No matter where you place the boundary between two values of a feature,
        the data will be split into the same left and right groups.<br />
        For this reason, CART considers only the <b>midpoint</b> between the
        values as a split candidate.<br />
        This avoids creating unnecessary boundaries and keeps the number of
        split candidates small but sufficient.
      </p>

      <h3>Q2: Why do we use a weighted value?</h3>
      <p>Consider the following example:</p>

      <pre>
Gini-left  = 0.1  
Gini-right = 0.2
</pre
      >

      <p>
        If we simply average these values, we get 0.15 — but this is not
        correct.<br />
        When the left and right nodes have different sizes, we must consider the
        number of samples in each node to make the comparison fair.
      </p>

      <p>CART therefore uses:</p>

      <ul>
        <li>The number of samples in the left node</li>
        <li>The number of samples in the right node</li>
      </ul>

      <p>
        as weights to compute the <b>weighted Gini</b>.<br />
        Larger nodes have more influence, and smaller nodes have less. This
        makes the evaluation properly balanced.
      </p>

      <h3>Q3: What happens when there are missing values?</h3>
      <p>
        CART uses a method called <b>Surrogate Split</b>. A surrogate split is a
        “backup question” used when the main feature has missing values.
      </p>

      <p>
        First, CART chooses the best split (main split).<br />
        Then it searches for other features that can repeat almost the same
        left/right division and registers them as surrogates.<br />
        When a missing value occurs, CART uses this surrogate split to decide
        which branch the sample should follow.
      </p>
    </div>
  </body>
</html>
